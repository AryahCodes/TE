{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import torch\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_num_threads(30)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "# FNN Basis Net\n",
    "HIDDEN_LAYER_FNN = 1\n",
    "NEURONS_FNN = 5\n",
    "\n",
    "# Domain Parameter\n",
    "X_COLLOC_POINTS = 100\n",
    "BOUNDARY_SCALE = 10e1\n",
    "X_END = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Generate Domain\n",
    "# Generate Collocation Points\n",
    "x = torch.linspace(0.0, X_END, X_COLLOC_POINTS, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Model\n",
    "\n",
    "# Define QPINN\n",
    "def circuit(x, basis=None):\n",
    "    # Embedding\n",
    "    if EMBEDDING == \"NONE\":\n",
    "        for i in range(N_WIRES):\n",
    "            qml.RY(x, wires=i)\n",
    "    elif EMBEDDING == \"CHEBYSHEV\":\n",
    "        for i in range(N_WIRES):\n",
    "            qml.RY(2*torch.arccos(x), wires=i)\n",
    "    elif EMBEDDING == \"TOWER_CHEBYSHEV\":\n",
    "        for i in range(N_WIRES):\n",
    "            qml.RY(2*(i+1)*torch.arccos(x), wires=i)\n",
    "    elif EMBEDDING == \"FNN_BASIS\":\n",
    "        for i in range(N_WIRES):\n",
    "            qml.RY(basis[i] * x, wires=i)\n",
    "    \n",
    "    # Variational ansatz\n",
    "    for i in range(N_LAYERS):\n",
    "        for j in range(N_WIRES):\n",
    "            qml.RX(theta[i,j,0], wires=j)\n",
    "            qml.RY(theta[i,j,1], wires=j)\n",
    "            qml.RZ(theta[i,j,2], wires=j)\n",
    "    \n",
    "        for j in range(N_WIRES - 1):\n",
    "            qml.CNOT(wires=[j, j + 1])\n",
    " \n",
    "    ## Cost Function\n",
    "    ## Z-Magnetization as cost function\n",
    "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(N_WIRES)]))\n",
    "\n",
    "\n",
    "# Define FNN for the basis\n",
    "class FNNBasisNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_layers, branch_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.branch_width = branch_width\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(torch.nn.Linear(1, branch_width))\n",
    "        for i in range(n_hidden_layers - 1):\n",
    "            self.layers.append(torch.nn.Linear(branch_width, branch_width))\n",
    "        self.layers.append(torch.nn.Linear(branch_width, N_WIRES))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            x = torch.tanh(self.layers[i](x))\n",
    "        x = self.layers[self.n_hidden_layers](x)\n",
    "        return x\n",
    "\n",
    "def model(x):\n",
    "    # Rescale input to [-0.95, 0.95]       \n",
    "    x_rescaled = 0.95 * 2*x - 0.95\n",
    "    \n",
    "    if EMBEDDING == \"FNN_BASIS\":\n",
    "        return circuit_qnode(x_rescaled.T, basisNet(x_rescaled.unsqueeze(1)).T)\n",
    "    else:\n",
    "        return circuit_qnode(x_rescaled.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the reference solution\n",
    "\n",
    "def derivatives_fnc(x, u):\n",
    "    du_dx = 4*u - 6*u**2 + math.sin(50*x) + u*math.cos(25*x) - 0.5\n",
    "    return du_dx\n",
    "\n",
    "reference_solution = torch.tensor(solve_ivp(derivatives_fnc, [0.0,X_END+0.000001], [0.75], t_eval=x.detach().cpu()).y, device=device)\n",
    "\n",
    "def compute_MSE_ref():\n",
    "    prediction = model(x)\n",
    "    return torch.mean((prediction-reference_solution)**2).detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the problem\n",
    "\n",
    "def loss_diff_fnc():\n",
    "    u = model(x) \n",
    "    du_dx = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    res = du_dx - (4*u - 6*u**2 + torch.sin(50*x) + u*torch.cos(25*x) - 0.5)\n",
    "\n",
    "    return torch.mean(res**2)\n",
    "\n",
    "def loss_boundary_fnc():\n",
    "    u_0 = model(torch.zeros_like(x))\n",
    "    return torch.mean((u_0 - 0.75)**2)\n",
    "\n",
    "def loss_fnc():\n",
    "\n",
    "    loss_diff     = loss_diff_fnc()\n",
    "    loss_boundary = loss_boundary_fnc()\n",
    "\n",
    "    return BOUNDARY_SCALE*loss_boundary + loss_diff\n",
    "\n",
    "def closure():\n",
    "    opt.zero_grad()\n",
    "    l = loss_fnc()\n",
    "    l.backward()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: FNN_BASIS \t Layers: 1 \t Qubits: 2\n",
      "Final Loss: 3.81E-01 \t Iteration: 38 \t Embedding: FNN_BASIS \t Layers: 1 \t Qubits: 2 \t Iterations: 38 \t MSE_ref 4.02E-04\n",
      "Embedding: FNN_BASIS \t Layers: 1 \t Qubits: 4\n",
      "Final Loss: 1.58E-03 \t Iteration: 49 \t Embedding: FNN_BASIS \t Layers: 1 \t Qubits: 4 \t Iterations: 49 \t MSE_ref 1.09E-05\n",
      "Embedding: FNN_BASIS \t Layers: 3 \t Qubits: 2\n",
      "Final Loss: 1.83E-01 \t Iteration: 49 \t Embedding: FNN_BASIS \t Layers: 3 \t Qubits: 2 \t Iterations: 49 \t MSE_ref 1.35E-04\n",
      "Embedding: FNN_BASIS \t Layers: 3 \t Qubits: 4\n",
      "Final Loss: 1.29E-04 \t Iteration: 49 \t Embedding: FNN_BASIS \t Layers: 3 \t Qubits: 4 \t Iterations: 49 \t MSE_ref 1.04E-05\n"
     ]
    }
   ],
   "source": [
    "## Benchmark different configurations\n",
    "\n",
    "EMBEDDING_LIST = [\"FNN_BASIS\", \"TOWER_CHEBYSHEV\", \"CHEBYSHEV\" ]\n",
    "\n",
    "data = np.zeros((5,4,2)) # layer, qubits, (loss, MSE_re)\n",
    "\n",
    "for EMBEDDING in EMBEDDING_LIST:\n",
    "    \n",
    "   for k,N_LAYERS in enumerate([1,3,5,7,10]):\n",
    "        for l,N_WIRES in enumerate([2, 4, 6, 8]):\n",
    "            print(f\"Embedding: {EMBEDDING} \\t Layers: {N_LAYERS} \\t Qubits: {N_WIRES}\")\n",
    "            \n",
    "            tmp_loss = []\n",
    "            tmp_mse_ref = []\n",
    "            \n",
    "            for i in range(500):\n",
    "                \n",
    "                circuit_qnode = qml.QNode(circuit, device=qml.device(\"default.qubit\", wires=N_WIRES))\n",
    "                theta = torch.rand(N_LAYERS, N_WIRES, 3, device=device, requires_grad=True)\n",
    "\n",
    "                if EMBEDDING == \"FNN_BASIS\":\n",
    "                    basisNet = FNNBasisNet(HIDDEN_LAYER_FNN, NEURONS_FNN).to(device)\n",
    "                    opt = torch.optim.LBFGS([theta, *basisNet.parameters()], line_search_fn=\"strong_wolfe\")\n",
    "                else:\n",
    "                    opt = torch.optim.LBFGS([theta], line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "                previous_loss = float('inf')\n",
    "                for i in range(50):\n",
    "                    opt.step(closure)\n",
    "                    print(f\"Epoch {i}, Loss: {loss_fnc().item():.2E}\", end=\"\\r\")\n",
    "\n",
    "                    if previous_loss == loss_fnc().item():\n",
    "                        break\n",
    "                    previous_loss = loss_fnc().item()\n",
    "                \n",
    "                tmp_loss.append(loss_fnc().item())\n",
    "                tmp_mse_ref.append(compute_MSE_ref())\n",
    "                \n",
    "            data[k,l,0] = np.mean(tmp_loss)\n",
    "            data[k,l,1] = np.mean(tmp_mse_ref)\n",
    "\n",
    "            print(f\"Final Loss: {loss_fnc().item():.2E} \\t Iteration: {i} \\t Embedding: {EMBEDDING} \\t Layers: {N_LAYERS} \\t Qubits: {N_WIRES} \\t Iterations: {i} \\t MSE_ref {compute_MSE_ref():.2E}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (teqpinn)",
   "language": "python",
   "name": "teqpinn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
