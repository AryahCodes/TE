{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_num_threads(30)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define QPINN\n",
    "def circuit(x, basis):\n",
    "    # Embedding\n",
    "    for i in range(N_QUBITS):\n",
    "        qml.RY(basis[i] * x, wires=i)\n",
    "    \n",
    "    # Variational ansatz\n",
    "    for i in range(N_LAYERS):\n",
    "        for j in range(N_QUBITS):\n",
    "            qml.RX(theta[i,j,0], wires=j)\n",
    "            qml.RY(theta[i,j,1], wires=j)\n",
    "            qml.RZ(theta[i,j,2], wires=j)\n",
    "    \n",
    "        for j in range(N_WIRES - 1):\n",
    "            qml.CNOT(wires=[j, j + 1])\n",
    "\n",
    "    ## Cost Function\n",
    "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(N_QUBITS)]))\n",
    "\n",
    "class FNNBasisNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_layers, branch_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.branch_width = branch_width\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(torch.nn.Linear(1, branch_width))\n",
    "        for i in range(n_hidden_layers -1):\n",
    "            self.layers.append(torch.nn.Linear(branch_width, branch_width))\n",
    "        self.layers.append(torch.nn.Linear(branch_width, N_QUBITS))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            x = torch.tanh(self.layers[i](x))\n",
    "        x = self.layers[self.n_hidden_layers](x)\n",
    "        return x\n",
    "\n",
    "def model(x):\n",
    "\n",
    "    # Rescale input to [-0.95, 0.95]       \n",
    "    x_rescaled = 0.95 * 2*x - 0.95\n",
    "    \n",
    "    return qnode(x_rescaled.T, basisNet(x_rescaled.unsqueeze(1)).T)\n",
    "\n",
    "def loss_diff_fnc():\n",
    "    x = torch.linspace(0.0, 1.0, 100, requires_grad=True, device=device)\n",
    "    u = model(x) \n",
    "    du_dx = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    res = du_dx - (4*u - 6*u**2 + torch.sin(50*x) + u*torch.cos(25*x) - 0.5)\n",
    "\n",
    "    return torch.mean(res**2)\n",
    "\n",
    "def loss_boundary_fnc():\n",
    "    x = torch.linspace(0.0, 1.0, 100, requires_grad=True, device=device)\n",
    "    u_0 = model(torch.zeros_like(x))\n",
    "    return torch.mean((u_0 - 0.75)**2)\n",
    "\n",
    "def loss_fnc():\n",
    "\n",
    "    loss_diff     = loss_diff_fnc()\n",
    "    loss_boundary = loss_boundary_fnc()\n",
    "\n",
    "    return 10E1*loss_boundary + loss_diff        \n",
    "\n",
    "\n",
    "def compute_loss(theta, basisNet):\n",
    "    \n",
    "    opt = torch.optim.LBFGS([theta, *basisNet.parameters()], line_search_fn=\"strong_wolfe\")\n",
    "    qnode = qml.QNode(circuit, dev)\n",
    "\n",
    "    loss_history = []\n",
    "    \n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        l = loss_fnc()\n",
    "        l.backward()\n",
    "        return l\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    for i in range(50):\n",
    "            opt.step(closure)\n",
    "            loss_history.append(loss_fnc().item())\n",
    "    print(f\"\\t Time: {datetime.datetime.now() - start}\")\n",
    "\n",
    "    return loss_fnc().item(), loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qubits = [2,4,8]\n",
    "layer = [4,8,12]\n",
    "n_runs = 3\n",
    "\n",
    "methods = [\"Random\", \"IdentityBlocks\", \"OneBlock\",\"Xavier Uniform\", \"Xavier Normal\"]\n",
    "data = []\n",
    "\n",
    "for N_QUBITS in qubits:\n",
    "    for N_LAYERS in layer:\n",
    "\n",
    "        print(f\"Qubits: {N_QUBITS}, Layers: {N_LAYERS}\")\n",
    "        dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
    "        qnode = qml.QNode(circuit, dev)\n",
    "        \n",
    "        for i in range(n_runs):\n",
    "            print(f\"Trial {i}\")\n",
    "            \n",
    "            # Random Parameter Initialization\n",
    "            if \"Random\" in methods:\n",
    "                basisNet = FNNBasisNet(1, 5).to(device)\n",
    "                theta = 2*torch.pi*torch.rand((N_LAYERS, N_QUBITS, 3),device=device)\n",
    "                theta.requires_grad = True\n",
    "                loss_value, loss_history = compute_loss(theta, basisNet)\n",
    "                for m, loss in enumerate(loss_history):\n",
    "                    data.append({\"qubits\": N_QUBITS,\n",
    "                                \"layers\": N_LAYERS,\n",
    "                                \"method\": \"Random\",\n",
    "                                \"loss\": loss,\n",
    "                                \"run\": i,\n",
    "                                \"iteration\": m,\n",
    "                                \"loss\": loss})\n",
    "            \n",
    "\n",
    "            # Compute identity block initialization\n",
    "            if \"IdentityBlocks\" in methods:\n",
    "                basisNet = FNNBasisNet(1, 5).to(device)\n",
    "                theta = 2*torch.pi*torch.rand(N_LAYERS, N_QUBITS, 3, device=device)\n",
    "                for j in range(N_LAYERS//2):\n",
    "                    theta[2*j,:,:] = -theta[2*j+1,:,:]\n",
    "                theta.requires_grad = True\n",
    "                loss_value, loss_history = compute_loss(theta, basisNet)\n",
    "                for m, loss in enumerate(loss_history):\n",
    "                    data.append({\"qubits\": N_QUBITS,\n",
    "                                \"layers\": N_LAYERS,\n",
    "                                \"method\": \"IdentityBlocks\",\n",
    "                                \"loss\": loss,\n",
    "                                \"run\": i,\n",
    "                                \"iteration\": m,\n",
    "                                \"loss\": loss})\n",
    "            \n",
    "\n",
    "            # Compute identity block initialization\n",
    "            if \"OneBlock\" in methods:\n",
    "                basisNet = FNNBasisNet(1, 5).to(device)\n",
    "                theta = 2*torch.pi*torch.rand(N_LAYERS, N_QUBITS, 3, device=device)\n",
    "                for j in range(N_LAYERS//2):\n",
    "                    theta[j,:,:] = -theta[N_LAYERS-j-1,:,:]\n",
    "                theta.requires_grad = True\n",
    "                loss_value, loss_history = compute_loss(theta, basisNet)\n",
    "                for m, loss in enumerate(loss_history):\n",
    "                    data.append({\"qubits\": N_QUBITS,\n",
    "                                \"layers\": N_LAYERS,\n",
    "                                \"method\": \"One IdentityBlock\",\n",
    "                                \"loss\": loss,\n",
    "                                \"run\": i,\n",
    "                                \"iteration\": m,\n",
    "                                \"loss\": loss})\n",
    "                    \n",
    "\n",
    "            # Xavier Uniform Initialization\n",
    "            if \"Xavier Uniform\" in methods:\n",
    "                basisNet = FNNBasisNet(1, 5).to(device)\n",
    "                bound = math.sqrt(6/(2*N_QUBITS))\n",
    "                theta = torch.distributions.uniform.Uniform(-bound,bound).sample([N_LAYERS,N_QUBITS,3]).to(device)\n",
    "                theta.requires_grad = True\n",
    "                loss_value, loss_history = compute_loss(theta, basisNet)\n",
    "                for m, loss in enumerate(loss_history):\n",
    "                    data.append({\"qubits\": N_QUBITS,\n",
    "                                \"layers\": N_LAYERS,\n",
    "                                \"method\": \"Xavier Uniform\",\n",
    "                                \"loss\": loss,\n",
    "                                \"run\": i,\n",
    "                                \"iteration\": m,\n",
    "                                \"loss\": loss})\n",
    "            \n",
    "\n",
    "            # Xavier Normal Initialization\n",
    "            if \"Xavier Normal\" in methods:\n",
    "                basisNet = FNNBasisNet(1, 5).to(device)\n",
    "                variance = 1/(2*N_QUBITS)\n",
    "                theta = torch.distributions.normal.Normal(0, variance).sample([N_LAYERS,N_QUBITS,3]).to(device)\n",
    "                theta.requires_grad = True\n",
    "                loss_value, loss_history = compute_loss(theta, basisNet)\n",
    "                for m, loss in enumerate(loss_history):\n",
    "                    data.append({\"qubits\": N_QUBITS,\n",
    "                                \"layers\": N_LAYERS,\n",
    "                                \"method\": \"Xavier Normal\",\n",
    "                                \"loss\": loss,\n",
    "                                \"run\": i,\n",
    "                                \"iteration\": m,\n",
    "                                \"loss\": loss})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Save the data frame to a CSV file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df.to_csv(f'init_methods_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "# Save the data frame to a CSV file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df.to_csv(f'init_methods_{timestamp}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
